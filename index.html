<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Bridging the Gap Between Perception and Reasoning in Computer Vision</title>

		<meta name="description" content="Thesis Proposal">
		<meta name="author" content="Matt Klawonn">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/blood.css" id="theme">
        <link rel="stylesheet" href="css/thesis_proposal.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>

         <script type="text/javascript" src="js/utils.js"></script>

        <script type="text/javascript" src="js/thesis_proposal.js"></script>
        
    


		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
                <section>
					<h3>Bridging the Gap Between Perception and Reasoning in Computer Vision</h3>
                    <hr>
					<h4>Matt Klawonn</h4>
                    <h4>Advisor: James Hendler</h4>
                    <img class="inline" style="vertical-align:middle" src="images/rpi_logo.png" />
                    <aside class="notes">__</aside>
				</section>

                <section>
                    <section>
                        <h3>Information Retrieval from Images</h3>
                        <img style="height:400px" src="images/number_of_images.png"/>
                        <p><small>Image courtesy of Business Insider</small></p>
                        <aside class="notes">__</aside>
                    </section>

                    <section data-background="images/mother_son_dance.jpg" data-background-size="contain" style="color:#fff">
                        <!-- div style="background:#000000;position:absolute;right:0px;width:300px;" -->
                        <div style="background:#3F3F31;border-radius:25px;position:absolute;right:-100px;width:400px;font-size:0.75em;">
                            <h3 style="color:#fff">Current Image Processing Tasks</h3>
                            <hr>
                            <ul>
                                <span class="fragment">
                                    <p><b>Object Detection</b></p>
                                    <p>Detect the objects, potentially with bounding boxes.</p>
                                </span>
                                <span class="fragment">
                                    <p><b>Image Captioning</b></p>
                                    <p>Produce a natural language description of the image.</p>
                                </span>
                                <span class="fragment">
                                    <p><b>Visual Question Answering</b></p>
                                    <p>Answer a natural language question about the image, using natural language.</p>
                                </span>
                            </ul>
                            <hr>
                            <p><small>Image courtesy of <a href="http://windycityweddingdance.com/wedding-dance-lessons/mother-son-dance-lessons">WCWD</a></small></p>
                        </div>
                        <aside class="notes">Here are a few perceptual tasks that attempt to exploit the vast amount of image data available today. Each of these tasks are useful in their own way.</aside>
                    </section>

                    <section data-background="images/mother_son_dance.jpg" data-background-size="contain" style="color:#fff">
                        <div style="background:#3F3F31;border-radius:25px;position:absolute;right:-100px;width:400px;font-size:0.75em;">
                            <h3 style="color:#fff">Perception vs Reasoning</h3>
                            <hr>
                                <span class="fragment">
                                    <p><b>Perception</b></p> 
                                    <p>Extract information immediately available from the image.</p>
                                    <p>Example: What are the people doing?</p>
                                </span>
                                <span class="fragment">
                                    <p><b>Reasoning</b></p> 
                                    <p>Extract information available through logical techniques.</p>
                                    <p>Who is the mother of the groom?</p>
                                </span>
                            <hr>
                            <p><small>Image courtesy of <a href="http://windycityweddingdance.com/wedding-dance-lessons/mother-son-dance-lessons">WCWD</a></small></p>
                        </div>

                        <aside class="notes">What if instead of listing objects, describing them in natural language, or asking a simple question answer that can be directly inferred from the image, you wanted something requiring more "reasoning"? Most current tasks are perceptive. Tasks that require reasoning could give you </aside>
                    </section>

                    <section>
                        <h2>Bridging the Gap</h2>
                        <img src="images/bridging_the_gap_1.png"/>
                        <aside class="notes">Currently perception and reasoning are separated, perception being where we are, and reasoning being where I want to go. I argue that the tasks, models, and data that currently comprise learning perceptual models in computer vision are not currently interoperable with the </aside>
                    </section>

                    <section>
                        <h2>Proposal</h2>
                        <blockquote>In this work, I define necessary conditions for an image representation generated by a perceptual model to be used by a reasoning system. My work proposes to learn perceptual models for the purpose of extracting information from images, while also yielding outputs suitable to be used with reasoning technologies.</blockquote>
                        <aside class="notes">__</aside>
                    </section>

                    <section>
                        <h2>Graph Structured Representation</h2>
                        <p>stokman1988 argues for the structuring of knowledge in graph form for the following reasons, and thus I adopt this approach.</p>
                        <ul>
                            <li class="fragment">Ease of Representation</li>
                            <li class="fragment">Clear Structure</li>
                            <li class="fragment">Access to Graph Theoretical Tools</li>
                        </ul>
                        <aside class="notes">This is the most explored area of current work since the task of Scene Graph Generation was proposed. I define this task later and explain why, as it stands, it produces an output almost suitable for use with reasoning technologies. However, it's not quite ready for a reasoning system.</aside>
                    </section>

                    <section>
                        <h2>Provenance Included</h2>
                        <p>Prior work (e.g cite mcguinness, cite lime) argues that trust is critical for the deployment of perceptual and reasoning models alike. <span class="fragment">As such I propose to build on previous work in machine learning explainability to capture an increased amount of provenance for a perceptual model's prediction.</span></p>
                        <aside class="notes">Doing so will facilitate the use of the model's output with a reasoning system.</aside>
                    </section>
                </section><!-- Intro Nest -->

                <section>
                    <h2>Talk Overview</h2>
                    <ul>
                        <li>Scene Graph Generation</li>
                        <small><ul>
                            <li>Outline of Task</li>
                            <li>Previous Work and Problems</li>
                            <li>More Flexible Approach</li>
                            <li>Future Improvements</li>
                        </ul></small>
                        <li>Provenance Generation</li>
                        <small><ul>
                            <li>ML and Explainability</li>
                            <li>Issues with Prior Approaches</li>
                            <li>Proof of Concept New Approach</li>
                        </ul></small>
                        <li>Discussion and Summary</li>
                    </ul>
                    <aside class="notes">First I'll talk about scene graph generation, where my current work has mostly been focused. I'll then talk about potential provenance generation methods and show a proof of concept.</aside>
                </section>

                <section>
                    <section>
                        <h2>Talk Overview</h2>
                        <ul style="color:#333">
                            <li style="color:#fff">Scene Graph Generation</li>
                            <small><ul style="color:#fff">
                                <li>Outline of Task</li>
                                <li>Previous Work and Problems</li>
                                <li>More Flexible Approach</li>
                                <li>Future Improvements</li>
                            </ul></small>
                            <li>Provenance Generation</li>
                            <small><ul>
                                <li>ML and Explainability</li>
                                <li>Issues with Prior Approaches</li>
                                <li>Proof of Concept New Approach</li>
                            </ul></small>
                            <li>Discussion and Summary</li>
                        </ul>
                        <aside class="notes">__</aside>
                    </section>


                    <section data-background="images/mother_son_dance.jpg" data-background-size="contain" style="color:#fff">
                        <!-- div style="background:#000000;position:absolute;right:0px;width:300px;" -->
                        <div style="background:#3F3F31;border-radius:25px;position:absolute;right:-100px;width:400px;font-size:0.75em;">
                            <h3 style="color:#fff">Scene Graph Generation</h3>
                            <hr>
                            <p>Scene graphs, proposed in johnson2015, offer a way to structure information from an image. <span class="fragment">A scene graph is a graph describing an image. Nodes can be objects or attributes. Edges are relationships between nodes.</span></p>
                            <!-- Change Image -->
                            <img src="images/motg_scene_graph.jpg"/>
                        </div>
                        <aside class="notes"> Scene graphs offer a mechanism to capture information from an image in an unambiguous and descriptive way. A scene graph is composed of nodes and edges corresponding to the content of the image. Each node-edge-node is called a triple, and triples can either describe the relationships between objects in the image, or attributes of objects in the image. Describes image content in a searchable way. Reduces ambiguity of natural language by making relationships more explicit via the graph's structure. Captures information from different parts of the scene. Most importantly, satisfies criteria 1.</aside>
                    </section>

                    <section>
                        <h2>Why Scene Graphs?</h2>
                        <ul>
                            <li>Ease of Representation</li>
                            <li>Clear Structure</li>
                            <li>Links Different Image Portions Together</li>
                            <li>Can Include Attributes and Relations</li>
                        </ul>
                        <aside class="notes">Scene graphs provide an easy way to say exactly how objects relate by labeling the edges of a graph. Further, they can unambiguously describe the entirety of an image.</aside>
                    </section>

                    <section>
                        <h2>Prior Works</h2>
                        <p>A number of works have attempted the difficult task of scene graph generation. There are a few problems with these works that fall into some common categories.</p>
                        <aside class="notes">__</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>No Attributes</h2>
                        <p>Some models do not generate a type of triple called attributes.</p>
                        <div style="position:relative; width:640px; height:410px; margin:0 auto;">
                            <img src="images/motg_scene_graph.jpg" height="400" style="position:absolute;top:0;left:50%;margin-left:-205px;"/>
                            <img class="fragment" src="images/motg_scene_graph_without_attributes.jpg" height="400" style="position:absolute;top:0;left:50%;margin-left:-205px;"/>
                        </div>
                        <aside class="notes">Our inference example presented earlier (about finding the mother of the groom) would not be possible without using attributes (i.e to determine that the groom is wearing white but the person he's dancing with is not).</aside>
                    </section>

                    <section>
                        <h2>Triples Only</h2>
                        <p>Some models only generate triples, and do not construct a graph. Further, this may be compounded with not producing attributes.</p>
                        <img src="images/triples_only.jpg"/>
                        <aside class="notes">Without the graph be properly structured, it's impossible to disambiguate objects with the same label.</aside>
                    </section>

                    <section>
                        <h2>Bounding Boxes Required</h2>
                        <p>Some models add a bounding box regression term to their loss functions.</p>
                        <img src="images/bbox_example.jpg"/>
                        <aside class="notes">Relying on bounding box information might not seem like such a big deal if the improvements in accuracy are very good. However, even with improvements in accuracy, realistically reliance on bounding boxes leads to reliance on a small set of training datasets since collecting large amounts of such data is time consuming and expensive. It then becomes easy to fall into the trap of beating a single dataset to death rather than evaluating on a wide range of data.</aside>
                    </section>

                    <section>
                        <h2>Overcoming Limitations</h2>
                        <p>With a method capable of generating <i>proper</i> scene graphs <i>with attributes</i> and <i>without using bounding boxes</i>, it would be possible to enable attribute based reasoning (mother of the groom) and construct datasets more easily. Towards the latter, there are natural language techniques for creating scene graphs from captions \citespice.</p>
                        <aside class="notes">Clearly there are some advantages to overcoming the issues that previous work has.</aside>
                    </section>

                    <section>
                        <h2>My Work</h2>
                        <p>cite klawonn2018, my work, proposes to overcome each of the outlined issues of prior work through various techniques.</p>
                        <aside class="notes">Everything I've talked about so far is prior work. What follows is a discussion of my work.</aside>
                    </section>

                    <section>
                        <h2>Generating Attributes</h2>
                        <img src="images/need_for_stochasticity.jpg" />
                        <aside class="notes">In order to generate attributes, a model must have some kind of stochasticity, since there can be multiple attributes associated with the same input.<aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Replacing Bounding Boxes</h2>
                        <h6>Which woman/man is it referring to?</h6>
                        <img src="images/need_for_attention.jpg" />
                        <aside class="notes">There's a potential problem with this approach, however, as illustrated by this picture. If our generator generates the triples shown, we need a way of determining if those two "person" nodes are referring to the same person or different people.</aside>
                    </section>

                    <section>
                        <h2>Solution: Attention</h2>
                        <p>Attention, proposed in xushow2015, spatially grounds predictions in the input image.</p>
                        <img src="images/attention_over_time.jpg.png" />
                        <p><small>Image courtesy of (Xu et al) 2015</small><p>
                        <aside class="notes">Here we have a visual representation of attention, where the light areas of the picture indicate what area of the image the model was looking at to generate the corresponding word. Knowing which area of the image was used to produce which word can help to resolve ambiguities.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Bottom Up Approach</h2>
                        <p>Producing a graph in a single prediction step seems difficult given the outlined requirements. Is it possible to learn to predict triples and construct a graph?</p>
                        <img src="images/bottom_up_approach.jpg" />
                        <aside class="notes">With these requirements, predicting an entire graph all at once seems like it would be difficult. For example, generating attributes will add a number of extra nodes to the graph. Also, without bounding boxes to regress some region proposal or attention mechanism, it would likely be difficult to train a model to predict all of them at once. The outlined requirements suggest learning to generate individual triples first, then constructing them into a graph.</aside>
                    </section>

                    <section>
                        <h2>Training Setup</h2>
                        <img src="images/generating_triples_adversarial_architecture.png" />
                        <aside class="notes">Therefore, when we incorporate attention into our generator, with each portion of each triple we have an associated attention vector telling us what portion of the image was used to generate that word. This will help us with graph construction. The full setup is then as follows. Train the generator and discriminator. Then when predicting a graph, have the generator predict some triples. Get the discriminator's score for these triples, and if the triple's score is not above some threshold, throw it away. Then construct the graph with the remaining triples, resolving objects via the attention mechanism.</aside>
                    </section>

                    <section>
                        <h2>Results: Measuring Performance</h2>
                        <p>Methods are evaluated using a recall at k metric (proposed in xuetal2017, higher is better): <br/><br/> $\frac{\left| \mbox{k Generated Triples } \cap \mbox{ Ground Truth Triples} \right|}{\left|\mbox{Ground Truth Triples}\right|}$</p>
                        <aside class="notes">The reasoning behind using recall instead of precision is that recall does not penalize for correct triples that do not appear in the ground truth. This is beneficial since none of the images in a dataset are likely to be fully annotated.</aside>
                    </section>

                    <section>
                        <h2>Results: Evaluation Dataset</h2>
                        <p>The first results follow exactly the split of Xu et Al, which has a vocabulary containing 50 relations and 150 objects. <span class="fragment">Note that this split does not contain attributes.</span></p>
                        <img src="images/visual_genome.jpg" />
                        <aside class="notes">This split uses the visual genome dataset, which contains over 100k training images paired with scene graphs.</aside>
                    </section>

                    <section>
                        <h2>Results: Models I "Beat"</h2>
                        <table style="font-size:20px">
                            <thead>
                                <tr>
                                    <th>Paper</th>
                                    <th>Approach</th>
                                    <th>Bounding Boxes</th>
                                    <th>Generates Attributes</th>
                                    <th>Generates Proper Graph</th>
                                    <th>R @ 50</th>
                                </tr>
                            </thead>
                            <tbody>

                                <tr class="fragment highlight-green">
                                    <td>Klawonn et Al 2018</td>
                                    <td>Triples to Graph</td>
                                    <td>No</td>
                                    <td>Yes</td>
                                    <td>Yes</td>
                                    <td>6.84</td>
                                </tr>

                                <tr>
                                    <td>Lu et Al 2016</td>
                                    <td>Visual Model, Language Priors</td>
                                    <td class="fragment highlight-green">No</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-red">0.32</td>
                                    
                                </tr>

                                <tr>
                                    <td>Xang et Al 2017</td>
                                    <td>"Parallel Pairwise Recurrent Fully Convolutional Networks"</td>
                                    <td class="fragment highlight-red">Yes</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-red">2.41</td>
                                </tr>

                                <tr>
                                    <td>Xu et Al 2017</td>
                                    <td>Iterative Message Passing</td>
                                    <td class="fragment highlight-red">Yes</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-green">Yes</td>
                                    <td class="fragment highlight-red">3.44</td>
                                </tr>
                            </tbody>
                        </table>

                        <aside class="notes">These are models that my model outperforms in terms of recall, and also hold other advantages over.</aside>

                    </section>

                    <section>
                        <h2>Results: Models I "Don't Beat"</h2>
                        <table style="font-size:20px">
                            <thead>
                                <tr>
                                    <th>Paper</th>
                                    <th>Approach</th>
                                    <th>Bounding Boxes</th>
                                    <th>Generates Attributes</th>
                                    <th>Generates Proper Graph</th>
                                    <th>R @ 50</th>
                                </tr>
                            </thead>
                            <tbody>

                                <tr>
                                    <td>Klawonn et Al 2018</td>
                                    <td>Triples to Graph</td>
                                    <td class="fragment highlight-green">No</td>
                                    <td class="fragment highlight-green">Yes</td>
                                    <td class="fragment highlight-green">Yes</td>
                                    <td class="fragment highlight-red">6.84</td>
                                </tr>

                                <tr>
                                    <td>Li et Al 2017</td>
                                    <td>Objects, Phrases, and Region Captions</td>
                                    <td class="fragment highlight-red">Yes</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-green">Yes</td>
                                    <td class="fragment highlight-green">10.72</td>
                                </tr>

                                <tr>
                                    <td>Newell et Al 2017</td>
                                    <td>Associative Embeddings</td>
                                    <td class="fragment highlight-red">Yes</td>
                                    <td class="fragment highlight-red">No</td>
                                    <td class="fragment highlight-green">Yes</td>
                                    <td class="fragment highlight-green">9.7</td>
                                </tr>
                            </tbody>
                        </table>
                        <aside class="notes">These are models that I don't beat in terms of recall, but still have advantages over in that I can generate attributes without bounding boxes.</aside>

                    </section>

                    <section>
                        <h2>Results: New Evaluation Split</h2>
                        <p>We also evaluate on a custom split, which has 300 objects, 100 relations, and 100 attributes. On this split, we achieve a R @ 50 of 2.47. <span class="fragment">This should be easy to improve.</span></p>
                        <aside class="notes">One of the ways to improve is have two separate generators, one for attributes and one for relations.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Example without Attributes</h2>
                        <img src="images/scene_graph_example_without_attributes.png" />
                        <aside class="notes">In all the examples I'll show, the generated scene graphs were limited to under 10 triples to keep things interpretable.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Example with Attributes</h2>
                        <img src="images/scene_graph_example_with_attributes.png" />
                        <aside class="notes">__</aside>
                    </section>

                    <section>
                        <h2>Potential Improvements</h2>
                        <ul>
                            <li>Train the CNN</li>
                            <li>Triple Relaxation</li>
                            <li>Train on New Data</li>
                        </ul>
                        <aside class="notes">Training the convolutional component (as opposed to using pre-trained weights) will likely result in the biggest performance increase, since one could imagine the features needed to detect relationships and attributes are different from those needed only to detect objects.</aside>
                    </section>

                    <section>
                        <h2>Bridging the Gap</h2>
                        <img src="images/bridging_the_gap_2.png"/>
                        <aside class="notes">With the ability to generate scene graphs as I have described, we're a little closer to being able to reason over the content of images. The graph based representation will form the basis of any reasoning task.</aside>
                    </section>

                    <section>
                        <h2>Improving Trust</h2>
                        <p>In order for these scene graphs to be used as input to a reasoning system of some kind, their outputs must be trustworthy. Next I'll explore how this can be achieved.</p>
                        <aside class="notes">Improving trust is important not only because it's nice to verify the outputs of a deep model, but when doing reasoning it's extra critical to see how conclusions were arrived upon.</aside>
                    </section>

                </section><!-- Scene Graph Nest -->

                <section>
                    <section>
                        <h2>Talk Overview</h2>
                        <ul style="color:#333">
                            <li>Scene Graph Generation</li>
                            <small><ul>
                                <li>Outline of Task</li>
                                <li>Previous Work and Problems</li>
                                <li>More Flexible Approach</li>
                                <li>Future Improvements</li>
                            </ul></small>
                            <li style="color:#fff">Provenance Generation</li>
                            <small><ul style="color:#fff">
                                <li>ML and Explainability</li>
                                <li>Issues with Prior Approaches</li>
                                <li>Proof of Concept New Approach</li>
                            </ul></small>
                            <li>Discussion and Summary</li>
                        </ul>
                    </section>

                    <section>
                        <h2>The Need for Provenance</h2>
                        <p>Prior work (e.g mcguinness2004explaining) argues that the key to trust is understanding. In order to trust the conclusions made by a reasoning system, they must be explained in such a way that they can be understood.</p>
                        <aside class="notes">There is a lot of previous work that supports this argument, please see the formal writeup for more references.</aside>
                    </section>

                    <section>
                        <h2>Without Provenance</h2>
                        <img src="images/explaining_conclusions_1.jpg" />
                        <aside class="notes">This diagram shows how far one could go in explaining the conclusions of a reasoning system were the reasoning system not given any information about how the graph was generated, aside from maybe a reference to the agent. Note that it doesn't capture anything about how the input graph was generated.</aside>
                    </section>

                    <section>
                        <h2>Previous ML Provenance</h2>
                        <p>The majority of prior work cite cite cite chooses to explain predictions by illustrating what features were important to the model. <span class="fragment">They tend not to say, however, what those features are, i.e assign semantics to them.</span></p>
                        <img src="images/lime_example.png" />
                        <p><small>From ribeiro2016</small></p>
                        <aside class="notes">Deep learning explanation methods are almost exclusively limited to pointing to specific features which were important for a prediction.</aside>
                    </section>

                    <section>
                        <h2>Feature Importance as Provenance</h2>
                        <img src="images/explaining_conclusions_2.jpg" />
                        <aside class="notes">This gives more information, but would still require a human in the loop to verify that it makes sense.</aside>
                    </section>

                    <section>
                        <h2>Need for Symbolic Provenance</h2>
                        <p>Symbolic provenance:</p>
                        <ul>
                            <li>Is queryable</li>
                            <li>Is usable by non-human agents</li>
                            <li>Could give more insight into the model than a visual explanation alone</li>
                        </ul>
                        <aside class="notes">In order to enable an autonomous agent to make a decision based on some reasoning, it would be very important to have the provenance be usable by a non human agent.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Using Attributes as Provenance</h2>
                        <p>It may be possible to use the attributes of a scene graph as provenance. However these are still being generated by a black box.</p>
                        <img src="images/explaining_conclusions_3.jpg" />
                        <aside class="notes">If the thing generating the provennace in itself isn't verifiably "trustworthy" the provenance isn't really trustworthy. It would be bettr to have a more transparent means of analyzing the deep model.</aside>
                    </section>

                    <section>
                        <h2>Analyzing Deep Models</h2>
                        <div style="position:relative; width:640px; height:410px; margin:0 auto;">
                            <img src="images/deep_learning_learn_features.jpg" height="400" style="position:absolute;top:0;left:50%;margin-left:-305px;"/>
                            <!-- Altered version with boxes outlining that the features are learned -->
                            <img class="fragment" src="images/DL_Features.png" height="400" style="position:absolute;top:0;left:50%;margin-left:-305px;"/>
                        </div>
                        <aside class="notes">It's been empirically shown (via visualizations) that sometimes a deep model's activations correspond to human recognizable featurs. We need a way to examine the structure of the internal deep model graph and see if we can map activations/weights to features.</aside>
                        <p><small>Image courtesy of <a href="http://www.slideshare.net/roelofp/python-for-image-understanding-deep-learning-with-convolutional-neural-nets">here</a></small></p>
                    </section>

                    <section>
                        <h2>Analyzing Deep Models</h2>
                        <p>We would like some way of analyzing nodes and groups of nodes to see if some of them map to human recognizable features.</p>
                    </section>

                    <section>
                        <h2>Using Spectral Graph Theory</h2>
                        <ul>
                            <li>Deep networks are graphs</li>
                            <li>Graph laplacians measure local changes in node values (discreet second derivative operator)</li>
                            <li>Laplacian eigenvectors and eigenvalues have been shown to capture graph properties (see, e.g cite cite cite)</li>
                        </ul>
                        <aside class="notes">Spectral graph theory is analagous to Fourier analysis on a function over a graph. As such the basis defined by the eigenvectors of a laplacian might yield information about the features within the graph.</aside>
                    </section>

                    <section>
                        <h2>Proposed Workflow</h2>
                        <ul>
                            <li>Train a deep network</li>
                            <li>Forward pass over a pair of images with only one feature in common</li>
                            <li>Compute Laplacian</li>
                            <li>Find common eigenvectors</li>
                            <li>Create a training example (eiegenvector-feature pair)</li> 
                        </ul>
                        <aside class="notes">What will determine whether this works or not is how human interpretable the features of a deep network actually are. I.e, do the features map well to those features which humans assign to a particular object.</aside>
                    </section>

                    <section>
                        <h2>Potential Evaluation</h2>
                        <p>There are a few ways to potentially evaluate this mapping from eigenvectors to features.</p>
                        <ul>
                            <li>Eigenvector-feature pair accuracy</li>
                            <li>Recall and precision (all eigenvectors to all listed recognizable features)</li>
                            <li>Separating classes of images using spectra (better sparation, better representation)</li>
                        </ul>
                        <aside class="notes">Pretty self explanatory</aside>
                    </section>

                    <section>
                        <h2>Proof of Concept</h2>
                        <p>Slide showing how this approach works so far</p>
                        <aside class="notes">__</aside>
                    </section>

                    <section>
                        <h2>Future Experimentation</h2>
                        <p>This slide will depend on how far I get with proof of concept</p>
                        <aside class="notes">__</aside>
                    </section>

                    <section>
                        <h2>Potential Roadblocks</h2>
                        <ul>
                            <li>Learned features may not interpretable enough</li>
                            <li>Deep model graphs may be too similar to be differentiated by spectra</li>
                        </ul>
                        <aside class="notes">It remains to be seen how well this will actually work.</aside>
                    </section>

                    <section>
                        <h2>Other Potential Provenance Generating Methods</h2>
                        <p>Bonus slide if I have time.</p>
                        <aside class="notes">__</aside>
                    </section>

                </section><!-- Provenance Generation Nest -->

                <section>

                    <section>
                        <h2>Talk Overview</h2>
                        <ul style="color:#333">
                            <li>Scene Graph Generation</li>
                            <small><ul>
                                <li>Outline of Task</li>
                                <li>Previous Work and Problems</li>
                                <li>More Flexible Approach</li>
                                <li>Future Improvements</li>
                            </ul></small>
                            <li>Provenance Generation</li>
                            <small><ul>
                                <li>ML and Explainability</li>
                                <li>Issues with Prior Approaches</li>
                                <li>Proof of Concept New Approach</li>
                            </ul></small>
                            <li style="color:#fff">Discussion and Summary</li>
                        </ul>
                    </section>
            
                    <section>
                        <h2>Symbolic Provenance</h2>
                        <p>Doing enough evaluation on the provenance generation is going to be critical to showing that it works. I will need a variety of models and data to test my laplacian approach, but I believe that there are sufficiently many data sets and model combinations to get a good evaluation.</p>
                        <aside class="notes">Obviously this is the more underdeveloped aspect of my work so far, so I'm less sure about it's success. That said, scene graph generation still has a ways to go.</aside>
                    </section>

                    <section>
                        <h2>SGG is Hard</h2>
                        <p>Scene Graph Generation is still very difficult, because it combines multiple tasks into one problem, such as object recognition, relationship detection, graph prediction, etc. <span class="fragment">The best systems still have a ways to go before their outputs are useable.</span></p>
                        <aside class="notes"> How models perform in more limited domains, however remains to be seen. It could be that they're good enough once the vocabulary and types of images have been limited.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Stretch Goal: Human-like navigation</h2>
                        <div style="position:relative; width:640px; height:410px; margin:0 auto;">
                            <img src="images/regular_gps.jpg" height="400" style="position:absolute;top:0;left:50%;margin-left:-205px;"/>
                            <img class="fragment" src="images/enhanced_gps.jpg" height="400" style="position:absolute;top:0;left:50%;margin-left:-205px;"/>
                        </div>
                        <aside class="notes">Now I'll talk about some tasks that could be accomplished using scene graphs. This figure shows how adding a scene graph could help improve HCI by giving an agent (like a GPS) access to the view of the world that the human sees. This was an actual route that I took and I had a lot of trouble finding my destination due to a lack of signage.</aside>
                    </section>

                    <!-- Change Image -->
                    <section>
                        <h2>Stretch Goal: Photojournalist Story Summary</h2>
                        <img src="images/photo_journalist.jpg" />
                        <aside class="notes">Using scene graphs may also allow a reasoning agent to reconstruct a story from a collection of images.</aside>
                    </section>

                    <section>
                        <h2>Tangential Work: Knowledge Base from Images</h2>
                        <p>With the ability to generate scene graph training data from captions, it may be possible to train a model in a weakly supervised setting. <span class="fragment">Given that this training process would have virtually unlimited training data, it would be interesting to allow it to run continuously on data from the Web and construct a simplistic "knowledge graph," or collection of knowledge about the relationships between and attributes of classes.</span></p>
                        <aside class="notes">__</aside>
                    </section>

                    <section>
                        <h2>Summary</h2>
                        <img src="images/bridging_the_gap_3.png" />
                        <aside class="notes">I have largely accomplished what I wanted to with my scene graph generation approach. I have only shown a small proof of concept for the task of generating symbolic provenance, so this is where the majority of my future work will go. Then once I "cross the bridge", I'd like to take a stab at an interesting application involving reasoning.</aside>
                    </section>

                    <section>
                        <h2>Questions?</h2>
                    </section>

                </section><!-- Discussion and stretch goals -->

            </div><!-- Slides div -->
		</div><!-- Reveal div -->

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				//progress: true,
				history: true,
				center: true,
                slideNumber: true,
                //showNotes: true,
                slideNumber: "c/t",

				transition: 'none', // none/fade/slide/convex/concave/zoom
                backgroundTransition: 'none',

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'
                },


				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>
	</body>
</html>
